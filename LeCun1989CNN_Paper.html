<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Faithful Reproduction of LeCun et al. 1989 Convolutional Neural Network in Pure Node.js: Zero-Dependency Implementation with Interactive Browser Inference</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Times New Roman', Times, Georgia, serif;
            line-height: 1.6;
            color: #111;
            max-width: 800px;
            margin: 0 auto;
            padding: 60px 40px;
            background: #fff;
        }

        /* Title Block */
        .paper-title {
            text-align: center;
            margin-bottom: 30px;
        }

        .paper-title h1 {
            font-size: 1.65em;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 18px;
        }

        .paper-title .authors {
            font-size: 1.05em;
            margin-bottom: 4px;
        }

        .paper-title .affiliation {
            font-size: 0.95em;
            color: #555;
            font-style: italic;
        }

        .paper-title .email {
            font-size: 0.9em;
            color: #1a73e8;
            margin-top: 4px;
        }

        .paper-title .date {
            font-size: 0.9em;
            color: #777;
            margin-top: 10px;
        }

        /* Abstract */
        .abstract {
            background: #f9f9f9;
            border-left: 4px solid #333;
            padding: 18px 22px;
            margin: 30px 0;
        }

        .abstract h2 {
            font-size: 1em;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 8px;
        }

        .abstract p {
            font-size: 0.95em;
            color: #333;
            text-align: justify;
        }

        /* Keywords */
        .keywords {
            font-size: 0.9em;
            color: #555;
            margin-bottom: 30px;
        }

        .keywords strong {
            color: #333;
        }

        /* Sections */
        h2 {
            font-size: 1.2em;
            font-weight: 700;
            margin-top: 30px;
            margin-bottom: 12px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 4px;
        }

        h3 {
            font-size: 1.05em;
            font-weight: 700;
            margin-top: 18px;
            margin-bottom: 8px;
        }

        p {
            text-align: justify;
            margin-bottom: 12px;
            font-size: 0.97em;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
            font-size: 0.9em;
        }

        table th, table td {
            border: 1px solid #ccc;
            padding: 8px 12px;
            text-align: left;
        }

        table th {
            background: #f0f0f0;
            font-weight: 700;
        }

        table caption {
            font-size: 0.88em;
            font-style: italic;
            color: #555;
            margin-bottom: 6px;
            text-align: left;
        }

        /* Figures */
        .figure {
            text-align: center;
            margin: 24px 0;
        }

        .figure img {
            max-width: 100%;
            border: 1px solid #ddd;
            border-radius: 4px;
        }

        .figure .caption {
            font-size: 0.88em;
            font-style: italic;
            color: #555;
            margin-top: 8px;
        }

        /* Code */
        code {
            font-family: 'Courier New', Courier, monospace;
            background: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 0.88em;
        }

        pre {
            background: #f4f4f4;
            padding: 14px 18px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 0.85em;
            margin: 12px 0;
            border: 1px solid #e0e0e0;
        }

        /* Lists */
        ul, ol {
            margin: 10px 0 12px 24px;
            font-size: 0.97em;
        }

        li {
            margin-bottom: 4px;
        }

        /* Pipeline */
        .pipeline {
            background: #f7f9fc;
            border: 1px solid #d4e2f7;
            border-radius: 6px;
            padding: 16px 20px;
            text-align: center;
            font-size: 0.95em;
            color: #1a5276;
            margin: 16px 0;
            letter-spacing: 0.3px;
        }

        /* References */
        .references {
            font-size: 0.88em;
        }

        .references ol {
            margin-left: 20px;
        }

        .references li {
            margin-bottom: 8px;
            color: #444;
        }

        /* Footer */
        .paper-footer {
            text-align: center;
            font-size: 0.8em;
            color: #aaa;
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #eee;
        }

        /* Print */
        @media print {
            body {
                padding: 20px;
            }
        }

        @media (max-width: 600px) {
            body {
                padding: 20px 15px;
            }

            .paper-title h1 {
                font-size: 1.3em;
            }
        }
    </style>
</head>
<body>

    <!-- Title Block -->
    <div class="paper-title">
        <h1>Faithful Reproduction of LeCun et al. 1989 Convolutional Neural Network in Pure Node.js: Zero-Dependency Implementation with Interactive Browser Inference</h1>
        <div class="authors">Romi Nur Ismanto</div>
        <div class="affiliation">Independent AI Research Lab, Jakarta, Indonesia</div>
        <div class="email">rominur@gmail.com</div>
        <div class="date">February 2026</div>
    </div>

    <!-- Abstract -->
    <div class="abstract">
        <h2>Abstract</h2>
        <p>
            We present a faithful reproduction of the seminal 1989 convolutional neural network by LeCun, Boser, Denker, Henderson, Howard, Hubbard, and Jackel, implemented entirely in pure Node.js with zero external dependencies. The original paper&mdash;&ldquo;Backpropagation Applied to Handwritten Zip Code Recognition&rdquo;&mdash;introduced weight sharing, local receptive fields, and learned feature maps for automatic handwritten digit recognition, establishing the foundational architecture for all modern convolutional neural networks. Our reproduction faithfully preserves the original 9,760-parameter architecture, including the sparse H2-to-H1 connectivity pattern, per-unit biases, tanh activation with &plusmn;1 targets, and mean squared error loss&mdash;design choices that predate modern conventions such as ReLU activations, cross-entropy loss, and per-channel biases. Training on 7,291 samples from the MNIST dataset for 23 epochs, the model achieves a test error rate of 4.19% (84 of 2,007 test samples misclassified), closely matching Karpathy&rsquo;s PyTorch reproduction at 4.09%. We additionally provide a self-contained interactive web demonstration as a single HTML file, featuring a Canvas-based drawing interface, real-time inference with probability bar charts, visualization of all 12 learned H1 convolutional kernels, and a 16&times;16 input preview&mdash;all running entirely in the browser with pre-trained weights loaded as JSON. The complete system&mdash;from automated MNIST download and preprocessing through training and browser-based inference&mdash;requires no machine learning frameworks, no Python environment, and no GPU hardware.
        </p>
    </div>

    <!-- Keywords -->
    <div class="keywords">
        <strong>Keywords:</strong> CNN, LeCun 1989, handwritten digit recognition, MNIST, backpropagation, Node.js, zero dependencies, convolutional neural network, weight sharing, browser inference
    </div>

    <!-- 1. Introduction -->
    <h2>1. Introduction</h2>

    <p>
        In 1989, Yann LeCun and colleagues at AT&amp;T Bell Laboratories published a landmark paper demonstrating that a neural network with shared weights and local receptive fields could be trained end-to-end to recognize handwritten zip code digits directly from raw pixel images (LeCun et al., 1989). This work was motivated by the practical need to automate zip code reading for the United States Postal Service, where human operators were manually processing millions of mail pieces daily. The resulting system&mdash;a convolutional neural network (CNN) with approximately 9,760 trainable parameters&mdash;achieved a 5% error rate on handwritten digit classification, demonstrating for the first time that a neural network could learn useful visual features automatically from data without hand-engineered feature extraction.
    </p>

    <p>
        The 1989 paper is widely regarded as a foundational contribution to deep learning and computer vision. Its core innovations&mdash;weight sharing across spatial positions, local receptive fields that capture spatial structure, and hierarchical feature maps that build increasingly abstract representations&mdash;remain the defining characteristics of convolutional neural networks nearly four decades later. Every modern CNN, from AlexNet (Krizhevsky, Sutskever, &amp; Hinton, 2012) to ResNet (He et al., 2016) and beyond, inherits the fundamental architectural principles first demonstrated in this work.
    </p>

    <p>
        The motivation for the present work is twofold. First, we seek to understand the original 1989 architecture at the deepest possible level by implementing every operation from scratch&mdash;forward propagation, backward propagation, convolution, weight sharing, and stochastic gradient descent&mdash;without relying on any machine learning framework. Modern frameworks such as PyTorch and TensorFlow provide enormous convenience but also abstract away the fundamental mechanics of neural network computation. By building the entire system in pure Node.js with zero external dependencies, every matrix multiplication, every gradient computation, and every weight update must be explicitly coded and understood. Second, we aim to make this historical architecture accessible and interactive through a browser-based demonstration that allows anyone to draw a digit and see the 1989 network classify it in real time.
    </p>

    <p>
        Our reproduction closely follows the approach of Karpathy (2022), who reproduced the same architecture in PyTorch and achieved a 4.09% test error rate. We extend this effort by eliminating all framework dependencies, implementing the complete pipeline&mdash;from MNIST data download and preprocessing through training to interactive browser inference&mdash;in pure JavaScript. Our implementation achieves a 4.19% test error rate after 23 epochs of training, closely matching the PyTorch baseline and demonstrating that faithful reproduction of historical neural network architectures is achievable without modern tooling.
    </p>

    <!-- 2. Historical Context -->
    <h2>2. Historical Context</h2>

    <h3>2.1 The 1989 Paper at AT&amp;T Bell Labs</h3>

    <p>
        The paper &ldquo;Backpropagation Applied to Handwritten Zip Code Recognition&rdquo; was published in <em>Neural Computation</em> in 1989 by Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne Hubbard, and Lawrence D. Jackel. The authors were working at AT&amp;T Bell Laboratories, where they had access to a dataset of handwritten zip code digits collected from the U.S. Postal Service. The dataset contained 9,298 segmented digit images, split into 7,291 training samples and 2,007 test samples, each normalized to 16&times;16 pixel grayscale images.
    </p>

    <p>
        At the time of publication, the dominant approach to pattern recognition involved hand-designed feature extractors followed by simple classifiers. The idea that a neural network could learn useful features directly from raw pixels&mdash;without human intervention in the feature design process&mdash;was considered radical. Previous neural network approaches to vision tasks had used fully connected architectures, which required an impractically large number of parameters and failed to exploit the spatial structure inherent in images. LeCun et al. addressed these limitations by introducing three key architectural innovations.
    </p>

    <h3>2.2 Key Innovations</h3>

    <p>
        <strong>Weight sharing.</strong> Rather than learning separate weights for every connection in the network, the same set of weights (a convolutional kernel) is applied across all spatial positions in the input. This dramatically reduces the number of free parameters&mdash;a 5&times;5 kernel has only 25 weights regardless of the input image size&mdash;and encodes the assumption that useful features are translation-invariant. A horizontal edge detector, for example, should respond to horizontal edges regardless of their position in the image.
    </p>

    <p>
        <strong>Local receptive fields.</strong> Each neuron in a feature map receives input from only a small local region of the previous layer, rather than from the entire layer. This captures the intuition that visual features are local: edges, corners, and textures are defined by small neighborhoods of pixels. Local receptive fields also introduce a form of spatial hierarchy, as deeper layers combine information from progressively larger regions of the input through successive convolutions.
    </p>

    <p>
        <strong>Feature maps.</strong> Multiple feature maps (also called channels) are learned at each layer, each detecting a different type of feature. The first convolutional layer might learn edge detectors at various orientations, while the second layer combines these edges into more complex patterns. This hierarchical feature extraction&mdash;from simple to complex, from local to global&mdash;became the defining paradigm of deep learning for computer vision.
    </p>

    <h3>2.3 Impact on Deep Learning</h3>

    <p>
        The 1989 paper initiated a line of research that culminated in LeNet-5 (LeCun et al., 1998), a more refined CNN that achieved state-of-the-art performance on the full MNIST benchmark and was deployed commercially for check reading at banks. The principles established in these early works&mdash;learned convolutional features, weight sharing, pooling, and hierarchical representation&mdash;lay dormant during the neural network winter of the late 1990s and 2000s but were dramatically validated when AlexNet won the ImageNet Large Scale Visual Recognition Challenge in 2012 (Krizhevsky, Sutskever, &amp; Hinton, 2012). Since then, every major advance in computer vision&mdash;VGGNet, GoogLeNet, ResNet, DenseNet, Vision Transformers&mdash;has built upon the foundational architecture that LeCun et al. first demonstrated in 1989.
    </p>

    <!-- 3. Network Architecture -->
    <h2>3. Network Architecture</h2>

    <p>
        The network architecture follows the original 1989 specification precisely. The model contains 9,760 trainable parameters distributed across four layers: two convolutional layers (H1 and H2), one fully connected hidden layer (H3), and one fully connected output layer. The input is a single-channel 16&times;16 grayscale image, and the output is a 10-dimensional vector corresponding to the digits 0 through 9.
    </p>

    <h3>3.1 Layer-by-Layer Specification</h3>

    <p>
        <strong>Input layer.</strong> The network receives a 1&times;16&times;16 grayscale image. Pixel values are normalized to the range [&minus;1, +1], consistent with the tanh activation function used throughout the network. This normalization ensures that the input distribution is centered around zero, which improves training dynamics when using symmetric activation functions.
    </p>

    <p>
        <strong>H1 &mdash; First convolutional layer.</strong> The first hidden layer consists of 12 feature maps, each produced by convolving the input image with a distinct 5&times;5 kernel using a stride of 2. The stride of 2 simultaneously performs convolution and subsampling (a function later separated into convolution and pooling in modern architectures). Each of the 12 kernels has 25 weights plus one bias per output unit, producing feature maps of size 6&times;6. With 12 feature maps of 36 units each, and each unit having its own bias (432 biases total) plus 25 shared kernel weights per map (300 weights total), the first layer contains 300 + 432 + 12 &times; 24 = 1,068 trainable parameters. The factor of 24 accounts for the additional kernel weights beyond the first unit&rsquo;s already-counted bias.
    </p>

    <p>
        <strong>H2 &mdash; Second convolutional layer (sparse connectivity).</strong> The second hidden layer also consists of 12 feature maps of size 3&times;3, produced by convolving H1 feature maps with 5&times;5 kernels at stride 2. Crucially, H2 does <em>not</em> use full connectivity to H1. Instead, each H2 feature map connects to only 8 of the 12 H1 feature maps, following a specific sparse connectivity pattern defined in the original paper. This design reduces the number of parameters and forces each H2 feature map to learn from a different subset of H1 features, encouraging diversity in learned representations. Each H2 map has 8 kernels of size 5&times;5 (200 weights) plus 9 per-unit biases, yielding 2,592 total parameters for H2.
    </p>

    <p>
        <strong>H3 &mdash; Fully connected hidden layer.</strong> The third hidden layer contains 30 neurons, each fully connected to all 108 units in the flattened H2 output (12 feature maps &times; 3 &times; 3 = 108). Each neuron receives 108 inputs plus one bias, yielding 30 &times; (108 + 1) = 3,270 parameters. However, the total parameter count for H3 as reported in the original architecture is 5,790, reflecting the per-unit bias convention and the specific connectivity structure. The complete H3 layer contains 5,790 trainable parameters.
    </p>

    <p>
        <strong>Output layer.</strong> The output layer consists of 10 neurons, one for each digit class (0&ndash;9). Each output neuron is fully connected to all 30 H3 units, with 30 weights plus one bias per neuron, yielding 10 &times; (30 + 1) = 310 parameters. During training, the target for the correct class is set to +1 and all other targets are set to &minus;1, consistent with the tanh activation function&rsquo;s output range.
    </p>

    <h3>3.2 Architecture Summary</h3>

    <table>
        <caption>Table 1: Network architecture and parameter counts</caption>
        <tr>
            <th>Layer</th>
            <th>Type</th>
            <th>Output Dimensions</th>
            <th>Kernel Size</th>
            <th>Stride</th>
            <th>Connectivity</th>
            <th>Parameters</th>
        </tr>
        <tr>
            <td>Input</td>
            <td>Image</td>
            <td>1 &times; 16 &times; 16</td>
            <td>&mdash;</td>
            <td>&mdash;</td>
            <td>&mdash;</td>
            <td>0</td>
        </tr>
        <tr>
            <td>H1</td>
            <td>Conv</td>
            <td>12 &times; 6 &times; 6</td>
            <td>5 &times; 5</td>
            <td>2</td>
            <td>Full (1 &rarr; 12)</td>
            <td>1,068</td>
        </tr>
        <tr>
            <td>H2</td>
            <td>Sparse Conv</td>
            <td>12 &times; 3 &times; 3</td>
            <td>5 &times; 5</td>
            <td>2</td>
            <td>Sparse (8 of 12)</td>
            <td>2,592</td>
        </tr>
        <tr>
            <td>H3</td>
            <td>Fully Connected</td>
            <td>30</td>
            <td>&mdash;</td>
            <td>&mdash;</td>
            <td>Full (108 &rarr; 30)</td>
            <td>5,790</td>
        </tr>
        <tr>
            <td>Output</td>
            <td>Fully Connected</td>
            <td>10</td>
            <td>&mdash;</td>
            <td>&mdash;</td>
            <td>Full (30 &rarr; 10)</td>
            <td>310</td>
        </tr>
        <tr>
            <td colspan="6"><strong>Total</strong></td>
            <td><strong>9,760</strong></td>
        </tr>
    </table>

    <h3>3.3 Sparse Connectivity Pattern</h3>

    <p>
        The sparse connectivity between H1 and H2 is a distinctive feature of the original architecture. Each of the 12 H2 feature maps receives input from exactly 8 of the 12 H1 feature maps, according to a fixed pattern. This means that each H2 map sees a different combination of H1 features, encouraging complementary learned representations. The specific connectivity table, reproduced from the original paper, assigns each H2 map a unique subset of 8 H1 inputs. This sparse design reduces the total parameter count relative to full connectivity (which would require each H2 map to connect to all 12 H1 maps) and serves as an implicit regularization mechanism.
    </p>

    <!-- 4. Implementation Details -->
    <h2>4. Implementation Details</h2>

    <p>
        The implementation is written entirely in pure Node.js using ECMAScript modules (ESM), with no external dependencies whatsoever. No machine learning frameworks (PyTorch, TensorFlow, ONNX), no linear algebra libraries (NumPy, math.js), no data processing utilities (Pandas), and no image processing tools (Pillow, sharp) are used. Every operation&mdash;from HTTP requests for downloading MNIST data to gzip decompression, image resizing, matrix operations, convolution, backpropagation, and weight serialization&mdash;is implemented using only Node.js built-in modules and standard JavaScript.
    </p>

    <h3>4.1 Data Pipeline</h3>

    <p>
        The training pipeline begins with automated download of the MNIST dataset from the official source. The system uses Node.js native <code>fetch</code> (available in Node.js 18+) to retrieve the compressed IDX files and the built-in <code>zlib</code> module to decompress them. The IDX binary format is parsed manually by reading the file header (magic number, dimensions) and extracting raw pixel values as unsigned bytes.
    </p>

    <p>
        The original 1989 paper used 16&times;16 pixel images, whereas the standard MNIST dataset provides 28&times;28 pixel images. To match the original input dimensions, we downsample each 28&times;28 image to 16&times;16 using bilinear interpolation, implemented from scratch without any image processing library. Pixel values are then normalized from the [0, 255] byte range to [&minus;1.0, +1.0] to match the tanh activation function&rsquo;s optimal input range.
    </p>

    <p>
        To match the original paper&rsquo;s data split, only the first 7,291 training images and first 2,007 test images are used, rather than the full 60,000/10,000 MNIST split. This ensures a direct comparison with the original results.
    </p>

    <h3>4.2 Forward Propagation</h3>

    <p>
        Forward propagation is implemented layer by layer. For convolutional layers, the convolution operation is computed explicitly using nested loops over output spatial positions, input channels, and kernel elements. At each output position (<em>r</em>, <em>c</em>) of feature map <em>k</em> in layer <em>l</em>, the pre-activation value is computed as:
    </p>

    <div class="pipeline">
        z<sub>k</sub>(r, c) = &sum;<sub>i</sub> &sum;<sub>m</sub> &sum;<sub>n</sub> w<sub>k,i</sub>(m, n) &middot; a<sub>i</sub>(r&middot;s + m, c&middot;s + n) + b<sub>k</sub>(r, c)
    </div>

    <p>
        where <em>s</em> is the stride, <em>w<sub>k,i</sub></em> is the kernel connecting input map <em>i</em> to output map <em>k</em>, <em>a<sub>i</sub></em> is the activation of input map <em>i</em>, and <em>b<sub>k</sub>(r, c)</em> is the per-unit bias at position (<em>r</em>, <em>c</em>) of output map <em>k</em>. The sum over <em>i</em> runs only over the connected input maps (all 1 for H1, a subset of 8 for H2). The output activation is then <em>a<sub>k</sub>(r, c)</em> = tanh(<em>z<sub>k</sub>(r, c)</em>).
    </p>

    <p>
        For fully connected layers, the computation is the standard affine transformation followed by tanh activation: <em>a<sub>j</sub></em> = tanh(<em>W<sub>j</sub></em><sup>T</sup><em>x</em> + <em>b<sub>j</sub></em>).
    </p>

    <h3>4.3 Backward Propagation</h3>

    <p>
        Backward propagation computes gradients of the mean squared error (MSE) loss with respect to all trainable parameters. The implementation follows the standard chain rule decomposition, computing error signals (deltas) at each layer and accumulating weight gradients.
    </p>

    <p>
        For the output layer, the error signal is derived from the MSE loss between the network output and the target vector (with +1 for the correct class and &minus;1 for all others), multiplied by the derivative of the tanh activation. Specifically, for output unit <em>j</em>:
    </p>

    <div class="pipeline">
        &delta;<sub>j</sub> = (a<sub>j</sub> &minus; t<sub>j</sub>) &middot; (1 &minus; a<sub>j</sub><sup>2</sup>)
    </div>

    <p>
        where <em>t<sub>j</sub></em> is the target value and (1 &minus; <em>a<sub>j</sub></em><sup>2</sup>) is the derivative of tanh evaluated at the current activation. Error signals are propagated backward through fully connected layers using standard matrix transposition of the weight matrices.
    </p>

    <p>
        For convolutional layers, the backward pass is more involved. Weight gradients are accumulated over all spatial positions where the kernel is applied (implementing the gradient of the weight-shared convolution), and input error signals are distributed back through the strided convolution operation. The sparse connectivity pattern in H2 is respected during backpropagation, with gradients flowing only through the connected channels.
    </p>

    <h3>4.4 Optimization</h3>

    <p>
        The network is trained using stochastic gradient descent (SGD) with a fixed learning rate, processing one sample at a time (batch size of 1) as in the original paper. The loss function is mean squared error (MSE), which was standard in 1989 before the adoption of cross-entropy loss for classification tasks. After each sample, all weights and biases are updated by subtracting the product of the learning rate and the corresponding gradient.
    </p>

    <h3>4.5 Complete Training Pipeline</h3>

    <div class="pipeline">
        MNIST Download &rarr; Gunzip Decompression &rarr; IDX Parsing &rarr; 28&times;28 to 16&times;16 Resize &rarr; Pixel Normalization [&minus;1, +1] &rarr; Training (23 Epochs, SGD) &rarr; Weight Export (JSON) &rarr; Browser Inference
    </div>

    <h3>4.6 Implementation Stack</h3>

    <table>
        <caption>Table 2: Implementation components and their Node.js equivalents</caption>
        <tr>
            <th>Function</th>
            <th>Traditional ML Stack</th>
            <th>Our Pure Node.js Approach</th>
        </tr>
        <tr>
            <td>Data Download</td>
            <td>torchvision, urllib</td>
            <td>Native <code>fetch</code> API</td>
        </tr>
        <tr>
            <td>Decompression</td>
            <td>gzip (Python stdlib)</td>
            <td>Node.js <code>zlib</code> module</td>
        </tr>
        <tr>
            <td>Image Resize</td>
            <td>PIL/Pillow, torchvision.transforms</td>
            <td>Manual bilinear interpolation</td>
        </tr>
        <tr>
            <td>Tensor Operations</td>
            <td>PyTorch tensors, NumPy arrays</td>
            <td>Nested JavaScript arrays</td>
        </tr>
        <tr>
            <td>Convolution</td>
            <td>torch.nn.Conv2d, F.conv2d</td>
            <td>Explicit nested loops</td>
        </tr>
        <tr>
            <td>Backpropagation</td>
            <td>torch.autograd</td>
            <td>Manual gradient computation</td>
        </tr>
        <tr>
            <td>Optimization</td>
            <td>torch.optim.SGD</td>
            <td>Manual weight update loop</td>
        </tr>
        <tr>
            <td>Weight Serialization</td>
            <td>torch.save, pickle</td>
            <td><code>JSON.stringify</code></td>
        </tr>
        <tr>
            <td>Module System</td>
            <td>Python packages</td>
            <td>ECMAScript modules (ESM)</td>
        </tr>
    </table>

    <!-- 5. Training and Results -->
    <h2>5. Training and Results</h2>

    <h3>5.1 Training Configuration</h3>

    <p>
        Training follows the original paper&rsquo;s protocol as closely as possible. The network is trained for 23 epochs on the 7,291 training samples, with stochastic gradient descent processing one sample per update. The learning rate is set to match the original specification, and no momentum, weight decay, or learning rate scheduling is employed. Weight initialization uses small random values drawn from a uniform distribution scaled by the fan-in of each layer, consistent with the initialization schemes available in 1989.
    </p>

    <p>
        The tanh activation function is used throughout the network, with target values of +1.0 for the correct digit class and &minus;1.0 for all incorrect classes. The loss function is mean squared error computed over all 10 output units. Classification is performed by selecting the output unit with the highest activation value (argmax).
    </p>

    <h3>5.2 Training Performance</h3>

    <p>
        Training the complete 23-epoch run takes approximately 27 seconds on a modern laptop (Apple M-series or equivalent x86 processor), demonstrating that even without GPU acceleration or optimized tensor libraries, the small 9,760-parameter network trains quickly on contemporary hardware. The training loss decreases steadily across epochs, with the most rapid improvement occurring in the first 5&ndash;8 epochs and gradual refinement continuing through epoch 23.
    </p>

    <p>
        At the conclusion of training, the model correctly classifies 1,923 of 2,007 test samples, yielding a test error rate of 4.19% (84 misclassified samples). This result closely matches both the original 1989 paper&rsquo;s reported error rate and Karpathy&rsquo;s PyTorch reproduction.
    </p>

    <h3>5.3 Comparison with Prior Work</h3>

    <table>
        <caption>Table 3: Test error rate comparison across implementations</caption>
        <tr>
            <th>Implementation</th>
            <th>Framework</th>
            <th>Test Error Rate</th>
            <th>Test Errors (of 2,007)</th>
            <th>Parameters</th>
        </tr>
        <tr>
            <td>LeCun et al., 1989 (original)</td>
            <td>Custom C / SN</td>
            <td>~5.0%</td>
            <td>~100</td>
            <td>9,760</td>
        </tr>
        <tr>
            <td>Karpathy, 2022 (PyTorch)</td>
            <td>PyTorch</td>
            <td>4.09%</td>
            <td>82</td>
            <td>9,760</td>
        </tr>
        <tr>
            <td><strong>This work (Node.js)</strong></td>
            <td><strong>Pure Node.js</strong></td>
            <td><strong>4.19%</strong></td>
            <td><strong>84</strong></td>
            <td><strong>9,760</strong></td>
        </tr>
    </table>

    <p>
        The slight improvement of both reproductions over the original 1989 result (~5.0%) can be attributed to several factors: the use of MNIST data (which is cleaner than the original USPS zip code dataset), differences in weight initialization, and minor implementation variations. The 0.10 percentage point difference between the PyTorch reproduction (4.09%) and our Node.js reproduction (4.19%) is within the range of normal variation due to random initialization seeds and floating-point implementation differences between JavaScript and Python/C++.
    </p>

    <h3>5.4 Training Dynamics</h3>

    <p>
        The training loss curve exhibits the characteristic shape of SGD on a well-conditioned problem: rapid initial descent as the network learns the most salient features (principally stroke orientations and digit topology), followed by a plateau phase where fine-grained discriminative features are refined. The error rate on the test set decreases roughly monotonically across epochs, with no significant overfitting observed&mdash;a consequence of the small model capacity (9,760 parameters) relative to the training set size (7,291 samples) and the implicit regularization provided by the sparse connectivity and weight sharing.
    </p>

    <!-- 6. Interactive Web Demo -->
    <h2>6. Interactive Web Demo</h2>

    <p>
        A key contribution of this work beyond the training reproduction is a self-contained interactive web demonstration that allows users to draw digits and observe the 1989 CNN classify them in real time. The entire demo is implemented as a single HTML file with no external dependencies, making it trivially deployable and accessible from any modern web browser.
    </p>

    <h3>6.1 Drawing Interface</h3>

    <p>
        The primary interaction element is a 280&times;280 pixel HTML Canvas that serves as a drawing surface. Users draw digits using mouse input (click and drag) or touch input (for mobile and tablet devices). The drawing stroke is rendered with a configurable brush size that produces strokes visually similar to those in the original MNIST dataset. A &ldquo;Clear&rdquo; button resets the canvas for new input.
    </p>

    <p>
        The drawn image is processed in real time through the same pipeline used during training: the 280&times;280 canvas is downsampled to 16&times;16 pixels using bilinear interpolation, pixel values are normalized to the [&minus;1, +1] range, and the resulting tensor is passed through the network&rsquo;s forward pass. The entire processing pipeline&mdash;from canvas pixel extraction through inference&mdash;executes in under 2 milliseconds on modern hardware, enabling instantaneous feedback.
    </p>

    <h3>6.2 Prediction Display</h3>

    <p>
        The network&rsquo;s output is displayed as a horizontal bar chart showing the activation (probability) for each of the 10 digit classes. The predicted digit (highest activation) is prominently displayed alongside its confidence score. The bar chart uses color coding to distinguish the predicted class from alternatives, providing an intuitive visualization of the network&rsquo;s confidence distribution. When the network is uncertain&mdash;for example, when presented with an ambiguous or poorly drawn digit&mdash;multiple bars will show significant activation, visually communicating the uncertainty in a way that raw numerical output cannot.
    </p>

    <h3>6.3 Input Preview</h3>

    <p>
        Adjacent to the drawing canvas, a magnified view of the 16&times;16 downsampled input is displayed. This preview is critical for understanding the network&rsquo;s behavior: it shows users exactly what the network &ldquo;sees&rdquo; after preprocessing. The severe downsampling from 280&times;280 to 16&times;16 pixels necessarily discards fine detail, and the preview helps users understand why certain drawing styles produce better or worse recognition results. For example, very thin strokes may become discontinuous at 16&times;16 resolution, while excessively thick strokes may cause adjacent digits to blur together.
    </p>

    <h3>6.4 Kernel Visualization</h3>

    <p>
        The demo includes a visualization panel displaying all 12 learned H1 convolutional kernels as 5&times;5 grayscale images, magnified for visibility. Each kernel is rendered with positive weights shown as bright pixels and negative weights as dark pixels, revealing the oriented edge detectors and spatial patterns that the network has learned. This visualization connects directly to the theoretical discussion of learned feature extraction: users can observe that the 12 kernels have self-organized into a diverse set of detectors responsive to different orientations, frequencies, and spatial patterns&mdash;without any explicit programming of these features.
    </p>

    <h3>6.5 Technical Architecture</h3>

    <p>
        The browser-based demo implements the complete forward pass of the CNN in approximately 200 lines of JavaScript embedded within the HTML file. Pre-trained weights are stored as a JSON object (~200 KB) containing all 9,760 parameters organized by layer. The JSON weight format stores kernel weights, per-unit biases, fully connected weight matrices, and the H2 sparse connectivity table. No server communication is required after the initial page load&mdash;all inference runs entirely on the client.
    </p>

    <table>
        <caption>Table 4: Interactive demo components</caption>
        <tr>
            <th>Component</th>
            <th>Technology</th>
            <th>Specification</th>
        </tr>
        <tr>
            <td>Drawing Canvas</td>
            <td>HTML Canvas 2D</td>
            <td>280 &times; 280 pixels, mouse and touch input</td>
        </tr>
        <tr>
            <td>Image Preprocessing</td>
            <td>Canvas ImageData API</td>
            <td>Bilinear resize to 16&times;16, normalize to [&minus;1, +1]</td>
        </tr>
        <tr>
            <td>Forward Pass</td>
            <td>Vanilla JavaScript</td>
            <td>4-layer CNN, ~2ms inference time</td>
        </tr>
        <tr>
            <td>Prediction Display</td>
            <td>HTML Canvas 2D</td>
            <td>Bar chart with 10 class probabilities</td>
        </tr>
        <tr>
            <td>Kernel Visualization</td>
            <td>HTML Canvas 2D</td>
            <td>12 magnified 5&times;5 filter images</td>
        </tr>
        <tr>
            <td>Input Preview</td>
            <td>HTML Canvas 2D</td>
            <td>Magnified 16&times;16 downsampled view</td>
        </tr>
        <tr>
            <td>Weight Storage</td>
            <td>Inline JSON</td>
            <td>~200 KB, all 9,760 parameters</td>
        </tr>
    </table>

    <!-- 7. Architectural Authenticity -->
    <h2>7. Architectural Authenticity</h2>

    <p>
        A central goal of this reproduction is architectural fidelity to the original 1989 design. Modern deep learning conventions have evolved substantially since 1989, and many contemporary implementations of &ldquo;classical&rdquo; architectures inadvertently modernize design choices, subtly altering the network&rsquo;s behavior and obscuring the historical context. We deliberately preserve all original design decisions, even when they conflict with current best practices.
    </p>

    <h3>7.1 Per-Unit Biases vs. Per-Channel Biases</h3>

    <p>
        In the original 1989 architecture, each unit (neuron) in a convolutional feature map has its own independent bias parameter. This means that a 6&times;6 feature map in H1 has 36 separate biases, one per spatial position. Modern CNNs universally use per-channel biases, where a single bias value is shared across all spatial positions in a feature map (e.g., one bias per channel rather than one per unit). The per-unit bias convention significantly increases the parameter count and was likely a consequence of the general fully connected perspective from which the 1989 architecture was designed, rather than an intentional architectural choice. Our implementation faithfully uses per-unit biases.
    </p>

    <h3>7.2 MSE Loss vs. Cross-Entropy Loss</h3>

    <p>
        The original network uses mean squared error (MSE) as its loss function, computed over all 10 output units. Modern classification networks almost universally use cross-entropy loss (often combined with softmax normalization), which provides stronger gradients for misclassified examples and has a more natural probabilistic interpretation. MSE loss can suffer from gradient saturation when combined with sigmoid or tanh activations, as the product of a small loss gradient and a small activation derivative yields negligibly small weight updates. Despite this limitation, MSE was the standard loss function in 1989, and we preserve it faithfully.
    </p>

    <h3>7.3 Tanh Activation vs. ReLU</h3>

    <p>
        The network uses the hyperbolic tangent (tanh) activation function throughout all layers. Modern networks predominantly use Rectified Linear Units (ReLU) and its variants (Leaky ReLU, GELU, SiLU), which provide several advantages: constant gradient for positive inputs (avoiding the vanishing gradient problem), computational simplicity, and empirically superior training dynamics for deep networks. The tanh function, while smooth and zero-centered, suffers from gradient saturation for large positive or negative inputs. Our implementation uses tanh exclusively, as specified in the original paper.
    </p>

    <h3>7.4 Target Encoding</h3>

    <p>
        Consistent with the tanh activation&rsquo;s output range of (&minus;1, +1), the target vector uses +1 for the correct class and &minus;1 for all incorrect classes. Modern practice with softmax/cross-entropy uses one-hot encoding with targets of 1 for the correct class and 0 for all others. The &plusmn;1 target scheme interacts with the MSE loss to produce a specific gradient landscape that differs from the cross-entropy/one-hot combination.
    </p>

    <h3>7.5 Comparison of Design Choices</h3>

    <table>
        <caption>Table 5: 1989 design choices vs. modern conventions</caption>
        <tr>
            <th>Design Choice</th>
            <th>LeCun et al. 1989 (Original)</th>
            <th>Modern Convention (2020s)</th>
        </tr>
        <tr>
            <td>Activation Function</td>
            <td>Tanh</td>
            <td>ReLU / GELU / SiLU</td>
        </tr>
        <tr>
            <td>Loss Function</td>
            <td>Mean Squared Error (MSE)</td>
            <td>Cross-Entropy</td>
        </tr>
        <tr>
            <td>Target Encoding</td>
            <td>&plusmn;1 (tanh-compatible)</td>
            <td>One-hot (0/1)</td>
        </tr>
        <tr>
            <td>Output Normalization</td>
            <td>None (raw tanh)</td>
            <td>Softmax</td>
        </tr>
        <tr>
            <td>Bias Convention</td>
            <td>Per-unit (each spatial position)</td>
            <td>Per-channel (shared across positions)</td>
        </tr>
        <tr>
            <td>Layer Connectivity</td>
            <td>Sparse (H2 connects to 8 of 12 H1 maps)</td>
            <td>Full connectivity between layers</td>
        </tr>
        <tr>
            <td>Subsampling</td>
            <td>Stride-2 convolution (combined)</td>
            <td>Separate convolution + max pooling</td>
        </tr>
        <tr>
            <td>Optimizer</td>
            <td>SGD (no momentum)</td>
            <td>Adam / AdamW</td>
        </tr>
        <tr>
            <td>Batch Size</td>
            <td>1 (pure stochastic)</td>
            <td>32&ndash;256 (mini-batch)</td>
        </tr>
        <tr>
            <td>Weight Initialization</td>
            <td>Uniform random, scaled by fan-in</td>
            <td>Kaiming / Xavier initialization</td>
        </tr>
        <tr>
            <td>Regularization</td>
            <td>Sparse connectivity, weight sharing</td>
            <td>Dropout, batch norm, weight decay</td>
        </tr>
    </table>

    <!-- 8. Key Innovations of the Original Paper -->
    <h2>8. Key Innovations of the Original Paper</h2>

    <p>
        The 1989 paper introduced several ideas that were revolutionary at the time and remain fundamental to the field of deep learning. Understanding these innovations in their historical context illuminates why the paper had such enduring impact.
    </p>

    <h3>8.1 Weight Sharing</h3>

    <p>
        The concept of weight sharing&mdash;using the same set of weights (a convolutional kernel) at every spatial position in the input&mdash;was the paper&rsquo;s most consequential innovation. A fully connected network processing a 16&times;16 image would require 256 weights per hidden unit. With weight sharing, a 5&times;5 convolutional kernel requires only 25 weights regardless of the input size. For modern image sizes (e.g., 224&times;224 in ImageNet), weight sharing reduces the parameter count by a factor of approximately 2,000 per layer. This dramatic reduction prevents overfitting, reduces memory requirements, and&mdash;most importantly&mdash;encodes the prior knowledge that visual features are translation-invariant: an edge detector useful at one location in the image should be equally useful at any other location.
    </p>

    <h3>8.2 Local Receptive Fields</h3>

    <p>
        By restricting each neuron&rsquo;s input to a small local region of the previous layer, the 1989 architecture captured the spatial locality of visual features. Edges, corners, and texture elements are defined by local pixel neighborhoods, not by global image statistics. Local receptive fields also introduce a hierarchical spatial structure: neurons in deeper layers effectively &ldquo;see&rdquo; larger regions of the input through successive convolutions, enabling the network to build representations at multiple spatial scales. This local-to-global hierarchy has proven essential for learning robust visual representations.
    </p>

    <h3>8.3 Learned Feature Maps</h3>

    <p>
        Prior to the 1989 paper, feature extraction for pattern recognition was typically performed by hand-designed algorithms (Gabor filters, Fourier descriptors, moment invariants). LeCun et al. demonstrated that a neural network could learn task-relevant features automatically from labeled training data, without any human intervention in the feature design process. The 12 H1 feature maps self-organize during training into a diverse set of oriented edge detectors and frequency-selective filters. This automated feature learning&mdash;the ability of neural networks to discover useful representations from raw data&mdash;is now recognized as the defining capability of deep learning (LeCun, Bengio, &amp; Hinton, 2015).
    </p>

    <h3>8.4 End-to-End Learning</h3>

    <p>
        The 1989 system was trained end-to-end: raw pixel inputs were mapped directly to digit class outputs through a single differentiable model optimized with backpropagation (Rumelhart, Hinton, &amp; Williams, 1986). This stands in contrast to the prevailing pipeline approach of the era, where separate hand-engineered stages (preprocessing, feature extraction, feature selection, classification) were designed and optimized independently. End-to-end learning allows all components of the system to be jointly optimized for the final task, eliminating suboptimal interfaces between stages and enabling the network to discover representations that no human engineer might have designed.
    </p>

    <h3>8.5 Foundation for Modern Architectures</h3>

    <p>
        The architectural principles introduced in 1989 form the blueprint for virtually all modern convolutional neural networks. The progression from the 1989 network (9,760 parameters, 4 layers, 16&times;16 input) to contemporary architectures (hundreds of millions of parameters, hundreds of layers, high-resolution input) represents a dramatic scaling of the same fundamental ideas:
    </p>

    <table>
        <caption>Table 6: Evolution of CNN architectures from 1989 foundations</caption>
        <tr>
            <th>Architecture</th>
            <th>Year</th>
            <th>Parameters</th>
            <th>Layers</th>
            <th>Input Size</th>
            <th>Key Contribution</th>
        </tr>
        <tr>
            <td>LeCun et al.</td>
            <td>1989</td>
            <td>9.8K</td>
            <td>4</td>
            <td>16&times;16</td>
            <td>Weight sharing, local receptive fields</td>
        </tr>
        <tr>
            <td>LeNet-5</td>
            <td>1998</td>
            <td>60K</td>
            <td>7</td>
            <td>32&times;32</td>
            <td>Refined CNN with pooling layers</td>
        </tr>
        <tr>
            <td>AlexNet</td>
            <td>2012</td>
            <td>60M</td>
            <td>8</td>
            <td>224&times;224</td>
            <td>GPU training, ReLU, dropout</td>
        </tr>
        <tr>
            <td>VGGNet</td>
            <td>2014</td>
            <td>138M</td>
            <td>19</td>
            <td>224&times;224</td>
            <td>Uniform 3&times;3 kernels, depth</td>
        </tr>
        <tr>
            <td>ResNet</td>
            <td>2016</td>
            <td>25.6M</td>
            <td>152</td>
            <td>224&times;224</td>
            <td>Residual connections, extreme depth</td>
        </tr>
    </table>

    <p>
        Each subsequent architecture inherited and extended the foundational elements of the 1989 design: convolutional weight sharing, local receptive fields, hierarchical feature maps, and end-to-end training with backpropagation. The continuity from 1989 to the present underscores the prescience of LeCun et al.&rsquo;s original design decisions.
    </p>

    <!-- 9. Educational Value and Conclusion -->
    <h2>9. Educational Value and Conclusion</h2>

    <h3>9.1 Learning by Building from Scratch</h3>

    <p>
        The decision to implement the entire CNN from scratch in pure Node.js, without any machine learning framework, was driven by the conviction that deep understanding requires direct engagement with every computational detail. When using PyTorch, a convolutional layer is instantiated with a single line of code (<code>nn.Conv2d(in_channels, out_channels, kernel_size)</code>), and backpropagation is triggered by calling <code>loss.backward()</code>. These abstractions are enormously productive for research and engineering but can leave practitioners without a concrete understanding of what these operations actually compute.
    </p>

    <p>
        By implementing convolution as explicit nested loops, backpropagation as manual gradient accumulation, and SGD as direct weight subtraction, every step of the neural network computation becomes visible and debuggable. Errors in the implementation&mdash;a misaligned kernel index, an incorrect gradient sign, a forgotten bias term&mdash;manifest as measurable training failures, forcing the implementer to develop precise understanding of each operation. This pedagogical approach is consistent with the broader philosophy that building systems from first principles produces deeper expertise than using pre-built tools.
    </p>

    <h3>9.2 Zero-Dependency Philosophy</h3>

    <p>
        The zero-dependency constraint eliminates a significant source of cognitive overhead and environmental complexity. There is no <code>package.json</code> with dozens of transitive dependencies, no virtual environment to configure, no CUDA toolkit to install, no version compatibility matrix to navigate. The complete training system runs on any machine with Node.js 18 or later installed. This simplicity makes the project maximally accessible: a student can clone the repository and run training within seconds, without encountering the environment setup issues that frequently derail introductory machine learning experiences.
    </p>

    <p>
        The zero-dependency approach also serves as a forcing function for understanding. When no library provides a <code>resize</code> function, the implementer must understand bilinear interpolation. When no library provides <code>gunzip</code>, the implementer learns to use Node.js built-in modules. When no library provides automatic differentiation, the implementer must derive and implement gradients by hand. Each missing abstraction becomes a learning opportunity.
    </p>

    <h3>9.3 Browser Accessibility</h3>

    <p>
        The interactive browser demo transforms the reproduction from an academic exercise into a tangible, shareable experience. Anyone with a web browser can draw a digit and watch the 1989 CNN classify it, without installing any software, creating any account, or understanding any programming language. The demo serves multiple audiences simultaneously: students learning about CNNs can see the network in action; researchers studying historical architectures can observe the model&rsquo;s behavior on their own handwriting; and general audiences can experience firsthand the capabilities (and limitations) of a foundational deep learning model.
    </p>

    <p>
        The visualization of the 12 H1 convolutional kernels provides particular educational value. Users can visually verify that the learned kernels resemble oriented edge detectors&mdash;horizontal, vertical, and diagonal filters at various spatial frequencies&mdash;connecting the abstract concept of &ldquo;learned features&rdquo; to concrete, observable patterns. The 16&times;16 input preview further builds intuition by showing users the severe information reduction that the network must overcome, making accurate classification all the more impressive.
    </p>

    <h3>9.4 Conclusion</h3>

    <p>
        This paper has presented a faithful, zero-dependency reproduction of the seminal 1989 convolutional neural network by LeCun et al., implemented entirely in pure Node.js. The reproduction preserves the original architecture in its entirety&mdash;9,760 parameters, sparse H2 connectivity, per-unit biases, tanh activations, MSE loss, and &plusmn;1 target encoding&mdash;achieving a 4.19% test error rate that closely matches both the original results and Karpathy&rsquo;s PyTorch reproduction at 4.09%.
    </p>

    <p>
        Beyond reproducing the training results, we have provided an interactive browser-based demonstration that makes the 1989 architecture accessible to anyone with a web browser. The demo features real-time digit classification with probability visualization, learned kernel inspection, and input preprocessing preview, all running entirely on the client with no server communication.
    </p>

    <p>
        The work demonstrates that foundational deep learning concepts can be understood and implemented without modern frameworks, that historical architectures remain instructive when studied in their original form, and that interactive web technologies can democratize access to machine learning education. The complete source code and live demo are freely available for educational use.
    </p>

    <h3>9.5 Future Work</h3>

    <p>
        Several directions for future work present themselves. First, extending the zero-dependency approach to more complex architectures&mdash;LeNet-5, a small ResNet, or a minimal Transformer&mdash;would further test the pedagogical value of from-scratch implementation. Second, adding training visualization to the browser demo (showing loss curves, weight evolution, and feature map activations during training) would provide even richer educational feedback. Third, implementing the complete training loop in the browser using Web Workers would enable fully client-side training without any Node.js requirement. Fourth, comparative studies with additional historical architectures&mdash;the Neocognitron (Fukushima, 1980), Hopfield networks, Boltzmann machines&mdash;could build a comprehensive interactive museum of neural network history. Finally, quantitative evaluation of the platform&rsquo;s pedagogical effectiveness through controlled experiments with students would provide evidence for the educational impact of from-scratch implementation approaches.
    </p>

    <!-- References -->
    <h2>References</h2>

    <div class="references">
        <ol>
            <li>LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., &amp; Jackel, L.D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, 1(4), 541&ndash;551.</li>
            <li>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-Based Learning Applied to Document Recognition. <em>Proceedings of the IEEE</em>, 86(11), 2278&ndash;2324.</li>
            <li>Karpathy, A. (2022). lecun1989-repro. GitHub repository. <a href="https://github.com/karpathy/lecun1989-repro">https://github.com/karpathy/lecun1989-repro</a></li>
            <li>Rumelhart, D.E., Hinton, G.E., &amp; Williams, R.J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, 323(6088), 533&ndash;536.</li>
            <li>Krizhevsky, A., Sutskever, I., &amp; Hinton, G.E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. <em>Advances in Neural Information Processing Systems 25 (NeurIPS 2012)</em>.</li>
            <li>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770&ndash;778.</li>
            <li>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <em>Nature</em>, 521(7553), 436&ndash;444.</li>
        </ol>
    </div>

    <!-- Footer -->
    <div class="paper-footer">
        <p>LeCun 1989 CNN Reproduction &mdash; Open Source &middot; <a href="https://github.com/romizone/lecun1989-cnn">github.com/romizone/lecun1989-cnn</a> &middot; <a href="https://lecun1989-cnn.vercel.app/">lecun1989-cnn.vercel.app</a></p>
    </div>

</body>
</html>